# 非线性激活函数
## Qestion 1
为什么要引入非线性激活函数？

如果不引入激活函数，这时候相当于激活函数`f(x)=x`，此时每一层的输出都是上一层输入的线性函数，这个时候，不论神经网络有多少层，输出都是输入的线性组合，那么隐藏层就失去了原始的作用，相当于最原始的感知机（Perceptron）了。

因此，引入非线性函数作为激活函数，就赋予了神经网络了意义，使其逼近任意函数。

最早的激活是`sigmoid`函数或者是`tanh`函数，输出有界，可以很容易作为下一层的输入，也有一定的仿生物学原理。

## Question 2
为什么引入Relu(修正线性单元)激活函数呢？

1. 采用`sigmoid`函数，因为是指数运算，计算量大，在反向传播求解误差梯度时，求导涉及除法。

2. 对于深层网络，`sigmoid`等反向传播时，在`sigmoid`接近饱和区时，变化太缓慢，导致倒数趋于0，在这种情况下会造成信息丢失，也即是梯度消失的情况，从而无法完成深度网络的训练。

3. Relu会使得一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存的关系，缓解了过拟合问题的发生。

目前最主流的做法：Relu之后再加一步Batch Normalization，尽可能地保证每一层网络的输入具有相同的分布。

[参考](https://blog.csdn.net/caimouse/article/details/67630435)